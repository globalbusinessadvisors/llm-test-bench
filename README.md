<div align="center">

# ğŸ§ª LLM Test Bench

**A comprehensive, production-ready framework for benchmarking, testing, and evaluating Large Language Models**

[![CI](https://img.shields.io/github/actions/workflow/status/globalbusinessadvisors/llm-test-bench/llm-benchmark.yml?branch=main&label=CI&logo=github)](https://github.com/globalbusinessadvisors/llm-test-bench/actions)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Rust Version](https://img.shields.io/badge/rust-1.75%2B-blue.svg)](https://www.rust-lang.org)
[![Crates.io](https://img.shields.io/badge/crates.io-v0.1.0-orange)](https://crates.io)

[Features](#-features) â€¢ [Quick Start](#-quick-start) â€¢ [Documentation](#-documentation) â€¢ [Architecture](#-architecture) â€¢ [Contributing](#-contributing)

</div>

---

## ğŸ“– Overview

LLM Test Bench is a powerful, enterprise-grade framework built in Rust for comprehensive testing, benchmarking, and evaluation of Large Language Models. It provides a unified interface to test multiple LLM providers, evaluate responses with sophisticated metrics, and visualize results through an intuitive dashboard.

### Why LLM Test Bench?

- **ğŸš€ Multi-Provider Support**: Test 14+ LLM providers with 65 models through a single, unified interface
- **ğŸ†• Latest Models**: Full support for GPT-5, Claude Opus 4, Gemini 2.5, and all 2025 releases
- **ğŸ“Š Comprehensive Metrics**: Evaluate models with perplexity, coherence, relevance, faithfulness, and custom evaluators
- **âš¡ High Performance**: Built in Rust for speed, safety, and scalability
- **ğŸ¨ Rich Visualization**: Interactive dashboards with real-time metrics and beautiful charts
- **ğŸ”Œ Extensible**: Plugin system, custom evaluators, and distributed computing support
- **ğŸ³ Production Ready**: Docker support, monitoring, REST/GraphQL APIs, and WebSocket streaming

---

## âœ¨ Features

### Core Capabilities

#### ğŸ¤– Multi-Provider LLM Support

**OpenAI (27 models)**
- **GPT-5** (Latest generation, Aug 2025)
- **GPT-4.5** (Feb 2025) - Enhanced capabilities
- **GPT-4.1** (Apr 2025) - Improved performance
- GPT-4o, GPT-4o-mini (Multimodal flagships)
- **o3-mini** (Jan 2025), o1, o1-preview, o1-mini (Advanced reasoning)
- GPT-4 Turbo, GPT-4 (128K context)
- GPT-3.5 Turbo

**Anthropic (15 models)**
- **Claude Opus 4** (May 2025 - Most capable)
- **Claude Sonnet 4.5** (Sept 2025 - Latest flagship)
- **Claude Sonnet 4** (May 2025)
- Claude 3.5 Sonnet, Claude 3.5 Haiku
- Claude 3 Opus, Sonnet, Haiku (200K context)

**Google Gemini (16 models)**
- **Gemini 2.5 Pro** (2025 - Latest generation)
- **Gemini 2.5 Computer Use** (Oct 2025 - Autonomous agent capabilities)
- Gemini 2.0 Flash (Extended thinking)
- Gemini 1.5 Pro, Flash (2M+ token context)
- Gemini Pro Vision (Multimodal)

**Mistral AI (7 models)**
- **Mistral Code** (Jun 2025 - Code-specialized)
- **Magistral Family** (Jun 2025 - Large, Medium, Small)
- **Voxtral Small** (Jul 2025 - Audio model)

**Additional Providers**
- **Azure OpenAI**: Enterprise-grade OpenAI models
- **AWS Bedrock**: Claude, Llama, Titan, and more
- **Open Source**: Ollama, Hugging Face, Together AI, Replicate
- **Specialized**: Cohere, Groq, Perplexity AI

#### ğŸ“ˆ Advanced Evaluation Metrics
- **Perplexity Analysis**: Statistical language model evaluation
- **Coherence Scoring**: Semantic consistency and logical flow
- **Relevance Evaluation**: Context-aware response quality
- **Faithfulness Testing**: Source attribution and hallucination detection
- **LLM-as-Judge**: Use LLMs to evaluate other LLMs
- **Text Analysis**: Readability, sentiment, toxicity, PII detection
- **Custom Evaluators**: Build your own evaluation metrics

#### ğŸ¯ Benchmarking & Testing
- **Systematic Testing**: Automated test suites with rich assertions
- **Comparative Analysis**: Side-by-side model comparison
- **Performance Profiling**: Latency, throughput, and cost tracking
- **A/B Testing**: Statistical significance testing for model selection
- **Optimization Tools**: Automatic parameter tuning and model recommendation

#### ğŸ“Š Visualization & Reporting
- **Interactive Dashboard**: Real-time metrics with Chart.js
- **Rich Charts**: Performance graphs, cost analysis, trend visualization
- **Multiple Formats**: HTML reports, JSON exports, custom templates
- **Cost Analysis**: Track spending across providers and models
- **Historical Trends**: Long-term performance tracking

#### ğŸŒ API & Integration
- **REST API**: Complete HTTP API with authentication
- **GraphQL**: Flexible query interface for complex data needs
- **WebSocket**: Real-time streaming and live updates
- **Monitoring**: Prometheus metrics and health checks
- **Distributed Computing**: Scale benchmarks across multiple nodes

#### ğŸ”Œ Extensibility
- **Plugin System**: WASM-based sandboxed plugins
- **Custom Evaluators**: Implement domain-specific metrics
- **Multimodal Support**: Image, audio, and video evaluation
- **Database Backend**: PostgreSQL with repository pattern
- **Flexible Architecture**: Clean, modular design for easy extension

---

## ğŸš€ Quick Start

### Prerequisites

- **Rust**: 1.75.0 or later ([Install Rust](https://rustup.rs/))
- **API Keys**: At least one LLM provider API key

### Installation

```bash
# Clone the repository
git clone https://github.com/globalbusinessadvisors/llm-test-bench.git
cd llm-test-bench

# Build the project
cargo build --release

# Install CLI globally (optional)
cargo install --path cli
```

### Configuration

Set up your API keys as environment variables:

```bash
# OpenAI
export OPENAI_API_KEY="sk-..."

# Anthropic
export ANTHROPIC_API_KEY="sk-ant-..."

# Google
export GOOGLE_API_KEY="..."

# AWS Bedrock
export AWS_ACCESS_KEY_ID="..."
export AWS_SECRET_ACCESS_KEY="..."
export AWS_REGION="us-east-1"
```

Or create a `.env` file:

```bash
cp .env.example .env
# Edit .env with your API keys
```

### Basic Usage

```bash
# Run a simple benchmark
llm-test-bench bench --provider openai --model gpt-4 --prompt "Explain quantum computing"

# Compare multiple models
llm-test-bench compare \
  --models "openai:gpt-4,anthropic:claude-3-sonnet" \
  --prompt "Write a Python function to sort a list"

# Analyze results
llm-test-bench analyze --results benchmark_results.json

# Launch interactive dashboard
llm-test-bench dashboard --port 8080

# Optimize model selection
llm-test-bench optimize \
  --metric latency \
  --max-cost 0.01 \
  --dataset prompts.json
```

### Docker Deployment

```bash
# Using Docker Compose (includes PostgreSQL, Redis, Prometheus)
docker-compose up -d

# Access the dashboard
open http://localhost:8080

# View metrics
open http://localhost:9090  # Prometheus
```

---

## ğŸ“š Documentation

### Getting Started
- [Quick Start Guide](docs/QUICKSTART_PHASE4.md) - Get up and running in 5 minutes
- [CLI Reference](docs/CLI_REFERENCE.md) - Complete command-line documentation
- [Configuration Guide](docs/CONFIGURATION.md) - Advanced configuration options

### Architecture & Design
- [Architecture Overview](docs/ARCHITECTURE_REPORT.md) - System design and components
- [Workspace Structure](docs/WORKSPACE_STRUCTURE.md) - Project organization
- [Technical Architecture](plans/PHASE5_TECHNICAL_ARCHITECTURE.md) - Deep dive into design

### Features
- [Provider Support](docs/PROVIDERS.md) - All supported LLM providers
- [API Documentation](docs/API.md) - REST & GraphQL API reference
- [Monitoring](docs/MONITORING.md) - Observability and metrics
- [Distributed Computing](docs/DISTRIBUTED.md) - Scaling across nodes
- [Multimodal](docs/MULTIMODAL.md) - Image, audio, and video support
- [Plugins](docs/PLUGINS.md) - Extensibility and custom plugins

### Deployment
- [Docker Deployment](docs/DOCKER_DEPLOYMENT.md) - Containerized deployment guide
- [Database Setup](docs/DATABASE.md) - PostgreSQL configuration

### Development
- [Phase Implementation Reports](docs/) - Detailed implementation history
- [Contributing Guide](CONTRIBUTING.md) - How to contribute
- [Development Setup](docs/DEVELOPMENT.md) - Set up your dev environment

---

## ğŸ—ï¸ Architecture

LLM Test Bench follows a clean, modular architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CLI Layer                            â”‚
â”‚  bench â”‚ compare â”‚ analyze â”‚ dashboard â”‚ optimize          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Core Library (core/)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Providers      â€¢ Evaluators     â€¢ Orchestration          â”‚
â”‚  â€¢ Analytics      â€¢ Visualization  â€¢ Monitoring             â”‚
â”‚  â€¢ Distributed    â€¢ Plugins        â€¢ Multimodal             â”‚
â”‚  â€¢ API Server     â€¢ Database       â€¢ Configuration          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    External Services                        â”‚
â”‚  LLM APIs â”‚ PostgreSQL â”‚ Redis â”‚ Prometheus â”‚ S3            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

- **Providers**: Unified interface for 14+ LLM providers
- **Evaluators**: Pluggable metrics for response quality assessment
- **Orchestration**: Intelligent routing, ranking, and comparison
- **Visualization**: Interactive dashboards and rich reporting
- **API Server**: REST, GraphQL, and WebSocket endpoints
- **Distributed**: Cluster coordination for large-scale benchmarks
- **Monitoring**: Prometheus metrics and health checks
- **Plugins**: WASM-based extensibility system

---

## ğŸ› ï¸ Technology Stack

- **Language**: Rust ğŸ¦€
- **CLI**: Clap (command-line parsing)
- **Async**: Tokio (async runtime)
- **HTTP**: Axum (web framework)
- **Database**: SQLx + PostgreSQL
- **Serialization**: Serde (JSON/YAML)
- **GraphQL**: Async-GraphQL
- **Monitoring**: Prometheus client
- **WebSocket**: Tokio-Tungstenite
- **Distributed**: Custom protocol over TCP
- **Plugins**: Wasmtime (WASM runtime)

---

## ğŸ“¦ Project Structure

```
llm-test-bench/
â”œâ”€â”€ cli/                    # Command-line interface
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ commands/      # CLI commands (bench, compare, etc.)
â”‚   â”‚   â””â”€â”€ main.rs
â”‚   â””â”€â”€ tests/             # Integration tests
â”œâ”€â”€ core/                   # Core library
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ providers/     # LLM provider implementations
â”‚   â”‚   â”œâ”€â”€ evaluators/    # Evaluation metrics
â”‚   â”‚   â”œâ”€â”€ orchestration/ # Model routing & comparison
â”‚   â”‚   â”œâ”€â”€ visualization/ # Dashboard & charts
â”‚   â”‚   â”œâ”€â”€ api/           # REST/GraphQL/WebSocket
â”‚   â”‚   â”œâ”€â”€ distributed/   # Cluster coordination
â”‚   â”‚   â”œâ”€â”€ monitoring/    # Metrics & health checks
â”‚   â”‚   â”œâ”€â”€ plugins/       # Plugin system
â”‚   â”‚   â”œâ”€â”€ multimodal/    # Image/audio/video
â”‚   â”‚   â”œâ”€â”€ analytics/     # Statistics & optimization
â”‚   â”‚   â””â”€â”€ config/        # Configuration
â”‚   â””â”€â”€ tests/             # Unit & integration tests
â”œâ”€â”€ docs/                   # Documentation
â”œâ”€â”€ examples/               # Usage examples
â”œâ”€â”€ plans/                  # Architecture & planning docs
â””â”€â”€ docker-compose.yml      # Docker deployment
```

---

## ğŸ¯ Use Cases

### 1. Model Selection
Compare multiple LLM providers to choose the best model for your use case based on quality, cost, and latency.

### 2. Quality Assurance
Systematic testing of LLM applications with rich assertions and automated evaluation metrics.

### 3. Performance Benchmarking
Measure and track latency, throughput, and cost across different models and configurations.

### 4. Regression Testing
Ensure model updates don't degrade quality with historical comparison and automated alerts.

### 5. Cost Optimization
Identify the most cost-effective model that meets your quality requirements.

### 6. Research & Experimentation
Rapid prototyping and comparison of different prompts, models, and parameters.

---

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Setup

```bash
# Clone and build
git clone https://github.com/globalbusinessadvisors/llm-test-bench.git
cd llm-test-bench
cargo build

# Run tests
cargo test

# Run with logging
RUST_LOG=debug cargo run -- bench --help

# Format code
cargo fmt

# Lint
cargo clippy -- -D warnings
```

### Areas for Contribution

- ğŸ”Œ New LLM provider integrations
- ğŸ“Š Additional evaluation metrics
- ğŸ¨ Visualization improvements
- ğŸ“ Documentation enhancements
- ğŸ› Bug fixes and performance improvements
- âœ¨ New features and capabilities

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- Built with [Rust](https://www.rust-lang.org/) ğŸ¦€
- Inspired by the need for comprehensive LLM testing tools
- Thanks to all contributors and the open-source community

---

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/globalbusinessadvisors/llm-test-bench/issues)
- **Discussions**: [GitHub Discussions](https://github.com/globalbusinessadvisors/llm-test-bench/discussions)
- **Documentation**: [docs/](docs/)

---

## ğŸ—ºï¸ Roadmap

### Completed âœ…
- âœ… Multi-provider LLM support (14+ providers)
- âœ… Advanced evaluation metrics
- âœ… Visualization dashboard
- âœ… REST/GraphQL/WebSocket APIs
- âœ… Distributed computing
- âœ… Monitoring & observability
- âœ… Plugin system
- âœ… Docker deployment
- âœ… PostgreSQL backend

### In Progress ğŸš§
- ğŸš§ Enhanced multimodal support
- ğŸš§ Advanced cost optimization
- ğŸš§ Plugin marketplace
- ğŸš§ Cloud deployment templates

### Planned ğŸ“‹
- ğŸ“‹ Real-time collaboration features
- ğŸ“‹ Advanced A/B testing framework
- ğŸ“‹ Integration with MLOps platforms
- ğŸ“‹ Enterprise SSO and RBAC

---

<div align="center">

**â­ Star us on GitHub â€” it motivates us a lot!**

[Report Bug](https://github.com/globalbusinessadvisors/llm-test-bench/issues) â€¢ [Request Feature](https://github.com/globalbusinessadvisors/llm-test-bench/issues) â€¢ [Documentation](docs/)

Made with â¤ï¸ by the LLM Test Bench Team

</div>
